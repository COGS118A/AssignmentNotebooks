{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d369f284",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"A2.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0cf8b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-68f2f0ed9883b594",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "\n",
    "## **Due: April 19th (Wednesday), 2023, 8:00pm (PT)**\n",
    "\n",
    "### **Instructions:**\n",
    "\n",
    "Your Jupyter notebook assignment will often have 3 elements: written answers, code answers, and quiz answers. For written answers, you may insert images of your handwritten work in code cells, or write your answers in markdown and LaTeX. For quiz answers, your `record.txt` file will record your answer choices in the quiz modules for submission. Both your quiz answers and code answers will be autograded on Gradescope. This assignment does not have the quiz portion.\n",
    "\n",
    "For all elements, DO NOT MODIFY THE CELLS. Put your answers **only** in the answer cells given, and **do not delete cells**. If you fail to follow these instructions, you will lose points on your submission.\n",
    "\n",
    "Make sure to show the steps of your solution for every question to receive credit, not just the final answer. You may search information online but you will need to write code/find solutions to answer the questions yourself. You will submit your .ipynb file and record.txt to gradescope when you are finished.\n",
    "\n",
    "### **Late Policy:**\n",
    "\n",
    "Late assignments will be accepted at 75% credit up to one week late. Consult the syllabus for more info on the late policy.\n",
    "\n",
    "### How to Include Your Math Written Answer?\n",
    "\n",
    "You could use inline $\\LaTeX$ in markdown (recommended) or use markdowns' include image functionality to submit your written responses.\n",
    "\n",
    "#### $\\LaTeX$ (recommended)\n",
    "[Here is a fantastic tutorial from CalTech about using $\\LaTeX$ in Jupyter Notebook.](http://chebe163.caltech.edu/2018w/handouts/intro_to_latex.html). You could also find various $\\LaTeX$ tutorials and cheat sheets online.\n",
    "\n",
    "#### Include Images\n",
    "If you are still getting familiar with using LaTeX, handwrite the response on paper or the stylus. Take a picture or screenshot of your answer, and include that image in the Jupyter Notebook. Be sure to include that image in the `\\imgs` directory. Let's say you have your Q1 response saved as `imgs/Q1.png`; the markdown syntax to include that image is `![Q1](imgs/Q1.png)`. \n",
    "\n",
    "## Important Notice\n",
    "\n",
    "You must check both submission output on the gradescope (`Assignment 2` and `Assignment 2 - Manual Grading`) correctly reflects your work and responses. If you notice inconsistencies between your notebook and the Manual Grading portion, you need to make a campuswire post, and we can help you with that.\n",
    "\n",
    "**Other**\n",
    "\n",
    "If you are not feeling comfortable with the programming assignments in this homework, it might help to take a look at [https://github.com/UCSD-COGS108/Tutorials](https://github.com/UCSD-COGS108/Tutorials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91823fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Run me\n",
    "!pip uninstall -y otter-grader\n",
    "!pip install -q  git+https://github.com/scott-yj-yang/otter-grader.git --no-warn-script-location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4500b2",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# Question 1: Argmin and Argmax\n",
    "\n",
    "An unknown estimator is given an estimation problem to find the minimizer and maximizer of the objective function $G(w) \\in (0, 20]$ : \n",
    "\n",
    "$$(w_a, w_b) = ( \\underset{w}{\\operatorname{argmin}}G(w), \\underset{w}{\\operatorname{argmax}} G(w))$$\n",
    "\n",
    "The solution to the above equation by the estimator is \n",
    "\n",
    "$$(w_a, w_b) = (4, 10)$$\n",
    "\n",
    "Given this information, please obtain the value of $w^*$ such that:\n",
    "\n",
    "$$w^∗ = \\underset{w}{\\operatorname{argmax}} [\\ln{G(w)} - 3.5/G(w)] $$\n",
    "\n",
    "Show your work as much as possible.\n",
    "\n",
    "_Points:_ 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394631ba",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116a740",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "# Question 2: Linear Regression\n",
    "\n",
    "Assume we are given a dataset $S = {(x_i, y_i) | i \\in {1, . . . , n}}$. Here, $x_i \\in \\mathbb{R}$ is a feature scalar (a.k.a. value of input variable) and $y_i \\in \\mathbb{R}$ is its corresponding value (a.k.a. value of dependent variable). In this section, we aim to fit data points with a line:\n",
    "\n",
    "$$ y = w_0 + w_1x $$\n",
    "\n",
    "where $w_0$, $w_1 \\in \\mathbb{R}$ are two parameters to determine the line. Next, we measure the quality of fitting by evaluating a sum-of-squares error function $g(w_0, w_1)$ :\n",
    "\n",
    "$$ g(w_0, w_1) = \\sum_{i=1}^{n} (w_0 + w_1 x_i - y_i)^2 $$\n",
    "\n",
    "When $g(w_0, w_1)$ is near zero, it means the proposed line can fit the dataset and model and model an accurate relation between $x_i$ and $y_i$. The best linewith parameters $(w^∗_0, w^∗_1)$ can reach the minimum value of the error function $g(w_0, w_1)$:\n",
    "\n",
    "$$(w^∗_0, w^∗_1) = \\underset{w_0, w_1}{\\operatorname{argmin}} g(w_0, w_1)$$\n",
    "\n",
    "To obtain the parameters of the best line, we will take the gradient of function $g(w_0, w_1)$ and set it to zero. That is:\n",
    "\n",
    "$$\\nabla g(w_0, w_1) = 0$$\n",
    "\n",
    "The solution $(w^*_0, w^*_1)$ of the above equation will determine the best line $y=w^∗_0 + w^∗_1 x$ that fits the dataset $S$.\n",
    "In reality, we typically tackle this task in a matrix form: First, we represent data points as matrices $X = [x_1, x_2, . . . , x_n]^T$ and $Y = [y1, y2, . . . , yn]^T$, where $x_i = [1, x_i]^T$ is a feature vector corresponding to $x_i$. The parameters of the line are also represented as a matrix $W = [w_0, w_1]^T$. Thus, the sum-of-squares error\n",
    "function $g(W)$ can be defined as (a.k.a. squared $L_2$ norm):\n",
    "\n",
    "$$\\begin{aligned} \n",
    "g(W) &= \\sum_{i=1}^{n}(x_i^TW - y_i)^2 \\\\\n",
    " &= || XW − Y ||_2^2 \\\\\n",
    " &= (XW − Y )^T(XW-Y)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Similarly, the parameters $W^∗ = [w^∗_0, w^∗_1]^T$ of the best line can be obtained by solving the equation below:\n",
    "\n",
    "$$ \\nabla g(W) = \\frac{\\partial g(W)}{\\partial W} = 0 $$\n",
    "\n",
    "Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e30632",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 2.1 OLS Matrix Form\n",
    "According to, compute the gradient of g(W) with respect to $W$. Your result should be in the form of $X$, $Y$ , and $W$. Show your work.\n",
    "\n",
    "_Points:_ 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d133885c",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962e51f",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 2.2:  OLS Matrix Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf9724",
   "metadata": {},
   "source": [
    "By setting the answer of last part to 0, prove the following:\n",
    "\n",
    "$$ W^∗ = \\underset{W}{\\operatorname{argmin}} g(W) = (X^TX)^{-1}X^TY$$\n",
    "\n",
    "Note: The above formula demonstrates a closed form solution of $$\\nabla g(W) = \\frac{\\partial g(W)}{\\partial W} = 0$$\n",
    "\n",
    "Show your work.\n",
    "\n",
    "_Points:_ 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502118f4",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4501d59e",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 3. Parabola Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2280c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Important packages (nothing to add to this cell)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bab7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (nothing to add to this cell)\n",
    "\n",
    "X_and_Y = np.load('./parabola_estimation.npy')\n",
    "old_X = X_and_Y[:, 0]  # Shape: (300,)\n",
    "Y = X_and_Y[:, 1]  # Shape: (300,)\n",
    "old_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c25fa7",
   "metadata": {},
   "source": [
    "## Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the original datapoints (nothing to add to this cell).\n",
    "\n",
    "def vis(w0, w1, w2):\n",
    "    draw_plane = (w0 is not None) and (w1 is not None) and (w2 is not None)\n",
    "    if draw_plane:\n",
    "        X_line = np.linspace(0,10,300)\n",
    "        Y_line = w0 + w1 * X_line + w2 * (X_line**2)\n",
    "        plt.plot(X_line, Y_line, color='orange')\n",
    "        \n",
    "    plt.scatter(old_X, Y, color='gray')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()\n",
    "    \n",
    "vis(None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f42dee",
   "metadata": {},
   "source": [
    "## Question 3.1: Parabola Estimation with Squared $L_2$ Norm\n",
    "Assume data points are represented as matrices $X$ and $Y$, please use the closed form solution to calculate the parameters $W$.\n",
    "\n",
    "In this section, you are constructing the design matrix $X$ and estimating the parameters $W$ $\\in$ $\\mathbb{R}^3$.\n",
    "\n",
    "The function that uses values of $W$ to estimate the parabola is:\n",
    "\n",
    "$$ y = w_0 + w_1 x + w_2 x^2$$\n",
    "\n",
    "You may use this formula to help you construct your design matrix. \n",
    "\n",
    "**Hint**: You may refer to Q2.2 for the analytic solution. Also, np.hstack may be useful for constructing the design matrix $X$.\n",
    "\n",
    "_Points:_ 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2816158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def l2_analytic_estimation(old_X, Y):\n",
    "    # Construct a design matrix of shape (300,3)\n",
    "    X = ...\n",
    "\n",
    "    # Hint: In the form of X and Y and should have a shape of (3,1)\n",
    "    W = ...\n",
    "\n",
    "    w0, w1, w2 = ...\n",
    "    return X, np.array([w0, w1, w2])\n",
    "\n",
    "X, W = l2_analytic_estimation(old_X, Y)\n",
    "w0,w1,w2 = W\n",
    "print('y = {:.2f} + {:.2f}*x + {:.2f}*x^2'.format(w0, w1, w2))\n",
    "vis(w0, w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15171512",
   "metadata": {},
   "source": [
    "## Question 3.2 Parabola Estimation with $L_1$  Norm\n",
    "\n",
    "In this problem, we would like to use the gradient descent to calculate the parameters $W$ for the parabola.\n",
    "If we have a loss function $\\mathcal{L}(W)$, then a typical gradient descent algorithm contains the following steps:\n",
    "\n",
    "**Step 1**. Initialize the parameters W.\n",
    "\n",
    "for i = 1 to `iterations`:\n",
    "\n",
    "- **Step 2**. Compute the gradient $\\nabla \\mathcal{L}(W) = \\frac{\\partial \\mathcal{L}(W)}{\\partial W}$.\n",
    "\n",
    "- **Step 3**. Update the parameters $W \\leftarrow \\mathcal{L}(W) = W - \\eta \\frac{\\partial \\mathcal{L}(W)}{\\partial W}$ where $\\eta$ is the learning rate.\n",
    "\n",
    "You may use your previous design matrix of $X$.\n",
    "\n",
    "\n",
    "_Points:_ 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c0bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gradient of L(W) with respect to W \n",
    "#(you need to add code to this cell as indicated below).\n",
    "\n",
    "def grad_L_W_ver1(X, Y, W):\n",
    "    grad = ...\n",
    "    assert grad.shape == (3,1)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056effa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimating W, which defines the hyperplane using gradient descent (you need to add code to this cell as indicated below).\n",
    "# y = w0 + w1*x + w2*x^2\n",
    "def l1_grad_descent(X, Y):\n",
    "    \n",
    "    assert X.shape == (300,3), f\"Your design matrix is shaped wrong, got {X.shape}\"\n",
    "    \n",
    "    # Some settings.\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    iterations    = 300000\n",
    "    learning_rate = 0.000001\n",
    "\n",
    "    # Gradient descent algorithm.\n",
    "    # Step 1. Initialize the parameters W using np.zeros\n",
    "    W = ...\n",
    "\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Step 2. Calculate the gradient of L(W) w.r.t. W. \n",
    "        grad = ...\n",
    "        # Step 3. Update parameters W.\n",
    "        ...\n",
    "\n",
    "    # Store the parameters of the parabola.\n",
    "    w0, w1, w2 = np.array(W).reshape(-1)\n",
    "    \n",
    "    return np.array([w0,w1,w2])\n",
    "\n",
    "# Visualization.\n",
    "w0, w1, w2 = l1_grad_descent(X,Y)\n",
    "print('y = {:.2f} + {:.2f}*x + {:.2f}*x^2'.format(w0, w1, w2))\n",
    "vis(w0, w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb86e92",
   "metadata": {},
   "source": [
    "## Question 3.3 Parabola Estimation with Squared $L_2$ Norm and $L_1$ Norm\n",
    "\n",
    "In this problem, we would like to use the gradient descent to calculate the parameters $W$ for the parabola.\n",
    "The loss function $\\mathcal{L}(W)$ now contains two parts: A squared $L_2$ norm and a $L_1$ norm.\n",
    "A coefficient $\\alpha$ is used to control the ratio of these two norms:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(W) \n",
    "& = \\sum_{i=1}^{n} \n",
    "\\Big(\\alpha\\big(\\mathbf{x}_i^T W - y_i\\big)^2 + (1-\\alpha)|\\mathbf{x}_i^T W - y_i| \\Big) \\\\\n",
    "& = \\alpha\\left\\lVert X W - Y \\right\\rVert_2^2 + (1-\\alpha)\\left\\lVert X W - Y \\right\\rVert_1 \\\\\n",
    "\\nonumber\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Note:** It may take 2~3 mins to run the algorithm.\n",
    "\n",
    "_Points:_ 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b4d96a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gradient of L(W) with respect to W (you need to add code to this cell as indicated below).\n",
    "\n",
    "def grad_L_W_ver2(X, Y, W, alpha):\n",
    "    ...\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de9ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to use gradient descent to estimate parabola given a list of alphas\n",
    "# (you need to add code to this cell as indicated below).\n",
    "# Hint: For each alpha, you need to use gradient descent, hence, you need to write a loop inside the loop.\n",
    "\n",
    "def l1_l2_grad_descent(old_X, X, Y, alpha_list):\n",
    "    # Some settings\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    iterations    = 300000\n",
    "    learning_rate = 0.000001\n",
    "    \n",
    "    w_history = []\n",
    "    # Loop over alpha(s).\n",
    "    ...\n",
    "        \n",
    "        # Gradient descent algorithm.\n",
    "        # Step 1. Initialize the parameters W.\n",
    "        W = ...\n",
    "        for i in range(iterations):\n",
    "            # Step 2. Calculate the gradient of L(W) w.r.t. W.\n",
    "            grad = ...\n",
    "            # Step 3. Update parameters W.\n",
    "            W = ...\n",
    "\n",
    "        # Get the parameters of the parabola.\n",
    "        w0, w1, w2 = np.array(W).reshape(-1)\n",
    "        w_history.append((w0,w1,w2))\n",
    "        \n",
    "    return np.array(w_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9d0c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimate the parabolas given the list of alpha(s).\n",
    "alphas = [0,0.03,0.05,0.1,1]\n",
    "w_history = l1_l2_grad_descent(old_X, X, Y, alpha_list=alphas)\n",
    "\n",
    "# print and plot the result\n",
    "plt.scatter(old_X, Y, color='gray')\n",
    "for alpha, ws in zip(alphas, w_history):\n",
    "    w0, w1, w2 = ws\n",
    "    \n",
    "    # plot\n",
    "    X_line = np.linspace(0,10,300)\n",
    "    Y_line = w0 + w1 * X_line + w2 * (X_line**2)\n",
    "    plt.plot(X_line, Y_line, label='alpha={}'.format(alpha))\n",
    "    \n",
    "    # print\n",
    "    print('When alpha = {},'.format(alpha))\n",
    "    print('y = {:.2f} + {:.2f}*x + {:.2f}*x^2'.format(w0, w1, w2))\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf16a004",
   "metadata": {},
   "source": [
    "# End of A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038d954",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit.\n",
    "\n",
    "Please make sure to see the output of the gradescope autograder. You are responsible for waiting and ensuring that the autograder is executing normally for your submission. Please create a campuswire post if you see errors in autograder execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f2520",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.export(pdf=False, force_save=True, run_tests=True, files=['imgs', 'parabola_estimation.npy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e47f6",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "parabola-l1": {
     "name": "parabola-l1",
     "points": 0.5,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "parabola-l1l2": {
     "name": "parabola-l1l2",
     "points": 0.5,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "parabola-l2": {
     "name": "parabola-l2",
     "points": 0.5,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
